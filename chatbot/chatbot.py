import pandas as pd
import re
import os
import math
import json
import random
from unidecode import unidecode
from rapidfuzz import fuzz
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

# ================= SYNONYM & KEYWORDS M·ªû R·ªòNG =================

SYNONYMS = {
    "√°o ph√¥ng": "√°o thun",
    "√°o ph√¥ng tay d√†i": "√°o thun d√†i tay",
    "√°o ·∫•m": "√°o len √°o n·ªâ √°o kho√°c √°o phao",
    "√°o kho√°c nh·∫π": "√°o kho√°c m·ªèng √°o gi√≥ cardigan",
    "ƒëi l√†m vƒÉn ph√≤ng": "ƒëi l√†m c√¥ng s·ªü vƒÉn ph√≤ng",
    "d·∫°o ph·ªë": "ƒëi ch∆°i d·∫°o ph·ªë h√†ng ng√†y",
    "ƒëi ti·ªác": "h·∫πn h√≤ date ti·ªác t√πng",
    "gym t·∫≠p gym": "th·ªÉ thao gym t·∫≠p th·ªÉ d·ª•c",
    "du l·ªãch ƒëi ch∆°i xa": "du l·ªãch travel ph∆∞·ª£t",
    "·ªü nh√† m·∫∑c nh√†": "·ªü nh√† b·ªô m·∫∑c nh√† ƒë·ªì ng·ªß",
    "tr·ªùi l·∫°nh m√πa ƒë√¥ng": "m√πa ƒë√¥ng tr·ªùi l·∫°nh r√©t",
    "n√≥ng b·ª©c oi b·ª©c": "h√® n√≥ng oi b·ª©c",
    # T·ª´ kh√≥a v√≠ ti·ªÅn
    "gi√†u c√≥": "nhi·ªÅu ti·ªÅn gi√†u sang sang ch·∫£nh high-end luxury",
    "kh√° gi·∫£": "trung b√¨nh kh√° gi·∫£ c√≥ ti·ªÅn ch√∫t",
    "ti·∫øt ki·ªám": "ngh√®o b√¨nh d√¢n √≠t ti·ªÅn ti·∫øt ki·ªám gi√° r·∫ª r·∫ª ti·ªÅn d∆∞·ªõi 500k",
}

INTENT_MAP = {
    "winter": ["m√πa ƒë√¥ng", "tr·ªùi l·∫°nh", "ƒë√¥ng", "l·∫°nh", "r√©t", "m√πa l·∫°nh", "th·ªùi ti·∫øt l·∫°nh", "m√πa r√©t"],
    "summer": ["m√πa h√®", "n√≥ng", "oi b·ª©c", "n√≥ng b·ª©c", "h√®", "th·ªùi ti·∫øt n√≥ng", "n·∫Øng n√≥ng"],
    "autumn": ["m√πa thu", "thu", "giao m√πa", "m√°t m·∫ª", "se l·∫°nh"],
    "spring": ["m√πa xu√¢n", "xu√¢n", "m√πa xu√¢n h√®", "xu√¢n h√®"],

    "work": ["ƒëi l√†m", "c√¥ng s·ªü", "vƒÉn ph√≤ng", "office", "ƒëi l√†m vƒÉn ph√≤ng", "l√†m vi·ªác"],
    "home": ["·ªü nh√†", "m·∫∑c nh√†", "·ªü nh√† m·∫∑c", "ngh·ªâ ng∆°i", "·ªü nh√† chill"],
    "casual": ["ƒëi ch∆°i", "d·∫°o ph·ªë", "ƒëi d·∫°o", "h√†ng ng√†y", "th∆∞·ªùng ng√†y", "daily"],
    "sport": ["th·ªÉ thao", "gym", "t·∫≠p gym", "ch·∫°y b·ªô", "t·∫≠p th·ªÉ d·ª•c", "sport"],
    "travel": ["du l·ªãch", "ƒëi ch∆°i xa", "ƒëi ph∆∞·ª£t", "ƒëi ch∆°i", "travel"],
    "date": ["h·∫πn h√≤", "ƒëi ti·ªác", "ti·ªác t√πng", "date", "ƒëi ch∆°i v·ªõi b·∫°n trai", "ƒëi ch∆°i v·ªõi b·∫°n g√°i", "ƒëi h·∫πn"]
}

CONTEXT_KEYWORDS = {
    "winter": ["√°o n·ªâ", "√°o len", "√°o kho√°c", "hoodie", "√°o phao", "√°o ·∫•m", "√°o d·∫°", "√°o kho√°c d√†y", "√°o l√¥ng"],
    "summer": ["√°o thun", "√°o ph√¥ng", "qu·∫ßn short", "v√°y", "ƒë·∫ßm", "ba l·ªó", "tank top", "√°o croptop"],
    "autumn": ["√°o kho√°c m·ªèng", "cardigan", "√°o d√†i tay", "√°o kho√°c nh·∫π", "√°o gi√≥", "√°o kho√°c bomber"],
    "spring": ["√°o s∆° mi", "√°o thun d√†i tay", "√°o kho√°c m·ªèng", "√°o blouse"],

    "work": ["√°o s∆° mi", "qu·∫ßn t√¢y", "vest", "qu·∫ßn √¢u", "√°o blouse", "√°o vest"],
    "home": ["b·ªô m·∫∑c nh√†", "ƒë·ªì ng·ªß", "pijama", "ƒë·ªì b·ªô", "ƒë·ªì m·∫∑c nh√†"],
    "casual": ["√°o thun", "hoodie", "jeans", "qu·∫ßn jogger", "qu·∫ßn short"],
    "sport": ["ƒë·ªì th·ªÉ thao", "gi√†y th·ªÉ thao", "qu·∫ßn short th·ªÉ thao", "legging", "√°o th·ªÉ thao"],
    "travel": ["√°o kho√°c nh·∫π", "qu·∫ßn short", "√°o thun", "balo", "gi√†y sneaker"],
    "date": ["v√°y", "ƒë·∫ßm", "√°o ki·ªÉu", "√°o hai d√¢y", "ch√¢n v√°y", "√°o croptop", "√°o √¥m"]
}

TOP_KEYWORDS = ["√°o", "hoodie", "len", "n·ªâ", "s∆° mi", "√°o kho√°c", "√°o thun", "√°o blouse"]
BOTTOM_KEYWORDS = ["qu·∫ßn", "jeans", "kaki", "v√°y", "short", "ƒë·∫ßm", "ch√¢n v√°y"]

PRODUCT_SCORES = {
    "√°o": 100,
    "hoodie": 95,
    "len": 95,
    "n·ªâ": 95,
    "√°o kho√°c": 90,
    "√°o thun": 90,
    "qu·∫ßn": 80,
    "v√°y": 85,
    "ƒë·∫ßm": 85,
    "gi√†y": 70,
    "ph·ª• ki·ªán": 40,
    "m≈©": 30,
    "ƒë·ªìng h·ªì": 30
}

UNISEX = ["unisex", "c·∫£ nam v√† n·ªØ", "nam n·ªØ"]

WALLET_LEVELS = {
    "gi√†u": ["gi√†u", "nhi·ªÅu ti·ªÅn", "sang ch·∫£nh", "high-end", "luxury", "kh√¥ng quan t√¢m gi√°", "kh√¥ng ng·∫°i gi√°"],
    "kh√°": ["kh√° gi·∫£", "trung b√¨nh", "kh√°", "c√≥ ti·ªÅn ch√∫t", "tr√™n 500k", "tr√™n 500"],
    "ti·∫øt ki·ªám": ["ti·∫øt ki·ªám", "ngh√®o", "b√¨nh d√¢n", "r·∫ª", "√≠t ti·ªÅn", "gi√° r·∫ª", "d∆∞·ªõi 500k", "d∆∞·ªõi 500"]
}

# ================= H√ÄM H·ªñ TR·ª¢ =================

def normalize_text(text: str) -> str:
    text = text.lower()
    text = unidecode(text)
    for key, value in SYNONYMS.items():
        text = text.replace(unidecode(key.lower()), unidecode(value.lower()))
    text = " ".join(text.split())
    return text

def fuzzy_match(keyword: str, text: str, threshold: int = 85) -> bool:
    norm_keyword = unidecode(keyword.lower())
    words = text.split()
    for word in words:
        if fuzz.ratio(norm_keyword, word) >= threshold:
            return True
    if fuzz.partial_ratio(norm_keyword, text) >= threshold:
        return True
    return False

def fuzzy_any(keywords: list[str], text: str) -> bool:
    norm_text = normalize_text(text)
    return any(fuzzy_match(k, norm_text) for k in keywords)

# ================= LOAD DB =================

DB_PATH = "chroma_db"
_VECTORDB_CACHE = None

def get_vectordb():
    global _VECTORDB_CACHE
    if _VECTORDB_CACHE is None:
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        _VECTORDB_CACHE = Chroma(
            persist_directory=DB_PATH,
            embedding_function=embeddings
        )
    return _VECTORDB_CACHE

# ================= LOAD DATA (Fallback n·∫øu DB l·ªói) =================

def load_products(csv_path="products.csv"):
    base = os.path.dirname(os.path.abspath(__file__))
    path = os.path.join(base, csv_path)

    df = pd.read_csv(path)
    df["price"] = pd.to_numeric(df["price"], errors="coerce")
    df = df.dropna(subset=["price"])
    df["price"] = df["price"].astype(int)

    def parse_size(val):
        try:
            return json.loads(val.replace("'", '"'))
        except:
            return []

    df["size"] = df["size"].apply(parse_size)
    return df.to_dict(orient="records")

_PRODUCTS_CACHE = None

def get_products_cached():
    global _PRODUCTS_CACHE
    if _PRODUCTS_CACHE is None:
        _PRODUCTS_CACHE = load_products()
    return _PRODUCTS_CACHE

# ================= SIZE INTENT =================

def detect_size_intent(text):
    text = normalize_text(text)
    return bool(re.search(r"\d+\s*cm", text) and re.search(r"\d+\s*kg", text))

def parse_height_weight(text):
    h = re.search(r"(\d+)\s*cm", text.lower())
    w = re.search(r"(\d+)\s*kg", text.lower())
    return (int(h.group(1)) if h else None, int(w.group(1)) if w else None)

def suggest_size(height, weight):
    if not height or not weight:
        return None
    if height >= 175:
        if weight <= 60: return "M"
        if weight <= 75: return "L"
        return "XL"
    if height >= 165:
        if weight <= 55: return "S"
        if weight <= 70: return "M"
        return "L"
    return "S"

def filter_by_size(products, size):
    if not size:
        return products
    return [p for p in products if size in p.get("size", [])]

# ================= INTENT DETECTION =================

def detect_contexts(text):
    norm_text = normalize_text(text)
    contexts = []
    for ctx, keys in INTENT_MAP.items():
        if fuzzy_any(keys, norm_text):
            contexts.append(ctx)
    return contexts

def detect_gender(text):
    norm_text = normalize_text(text)
    if fuzzy_any(["nam", "con trai", "d√†nh cho nam", "nam gi·ªõi"], norm_text):
        return "nam"
    if fuzzy_any(["n·ªØ", "con g√°i", "d√†nh cho n·ªØ", "n·ªØ gi·ªõi"], norm_text):
        return "n·ªØ"
    if fuzzy_any(["tr·∫ª em", "b√©", "tr·∫ª con", "kid", "em b√©"], norm_text):
        return "tr·∫ª em"
    return None

def detect_budget(text):
    norm_text = normalize_text(text)
    m = re.search(r"(\d+)\s*(k|tr|trieu|c·ªß)", norm_text)
    if not m:
        return None
    value = int(m.group(1))
    unit = m.group(2)
    return value * (1_000_000 if unit in ["tr", "trieu", "c·ªß"] else 1_000)

def detect_wallet_level(text):
    norm_text = normalize_text(text)
    if fuzzy_any(WALLET_LEVELS["gi√†u"], norm_text):
        return "gi√†u"
    if fuzzy_any(WALLET_LEVELS["kh√°"], norm_text):
        return "kh√°"
    if fuzzy_any(WALLET_LEVELS["ti·∫øt ki·ªám"], norm_text):
        return "ti·∫øt ki·ªám"
    return None

# ================= FILTER =================

def filter_by_gender(products, gender):
    if not gender:
        return products
    filtered = []
    for p in products:
        cat = p["category"].lower()
        name = p["name"].lower()
        if gender in cat or gender in name:
            filtered.append(p)
            continue
        if any(u in cat or u in name for u in UNISEX):
            filtered.append(p)
    return filtered

def filter_by_budget(products, budget):
    if not budget:
        return products
    return [p for p in products if p["price"] <= budget]

def filter_by_contexts(products, contexts):
    if not contexts:
        return products
    result = products
    for ctx in contexts:
        keys = CONTEXT_KEYWORDS.get(ctx)
        if not keys:
            continue
        tmp = []
        for p in result:
            if any(fuzzy_match(k, normalize_text(p["name"]), threshold=80) for k in keys):
                tmp.append(p)
        if tmp:
            result = tmp
    return result if result else products

# ================= SCORE & SORT =================

def score_product(p):
    name = normalize_text(p["name"])
    cat = p["category"].lower()
    score = 50
    for k, v in PRODUCT_SCORES.items():
        if fuzzy_match(k, name) or k in cat:
            score = max(score, v)
    return score

def sort_products(products):
    for p in products:
        p["score"] = score_product(p)
    return sorted(products, key=lambda x: x["score"], reverse=True)

# ================= COMBO & OUTFIT TH√îNG MINH =================

def get_price_category(total_price):
    if total_price >= 1_000_000:
        return "gi√†u", "sang tr·ªçng, ch·∫•t l∆∞·ª£ng cao c·∫•p, ƒë·∫≥ng c·∫•p"
    elif total_price >= 500_000:
        return "kh√°", "hi·ªán ƒë·∫°i, ch·∫•t l∆∞·ª£ng t·ªët, phong c√°ch"
    else:
        return "ti·∫øt ki·ªám", "tr·∫ª trung, nƒÉng ƒë·ªông, gi√° c·ª±c h·ªùi, ngh√®o, kh√≥ khƒÉn"

def build_smart_outfit(products, contexts, budget=None):
    if not products:
        return []

    sorted_prods = sorted(products, key=lambda x: x["price"])
    outfit = []
    total = 0
    max_budget = budget * 0.9 if budget else float('inf')

    has_winter = "winter" in contexts
    has_spring_autumn = any(c in contexts for c in ["spring", "autumn"])
    has_work = "work" in contexts
    has_date = "date" in contexts

    used_ids = set()

    # 1. Ch·ªçn TOP
    top_candidates = [p for p in sorted_prods if p["id"] not in used_ids
                      if any(fuzzy_match(k, normalize_text(p["name"])) for k in TOP_KEYWORDS)]
    if has_work:
        top_candidates = sorted(top_candidates,
                                key=lambda p: 100 if fuzzy_match("s∆° mi", normalize_text(p["name"])) or "blouse" in p["name"].lower() else 0,
                                reverse=True)
    if has_date:
        top_candidates = sorted(top_candidates,
                                key=lambda p: 100 if "√°o ki·ªÉu" in p["name"].lower() or "croptop" in p["name"].lower() else 0,
                                reverse=True)

    top = next((p for p in top_candidates if total + p["price"] <= max_budget), None)
    if top:
        outfit.append(top)
        total += top["price"]
        used_ids.add(top["id"])

    # 2. Ch·ªçn BOTTOM
    bottom_candidates = [p for p in sorted_prods if p["id"] not in used_ids
                         if any(fuzzy_match(k, normalize_text(p["name"])) for k in BOTTOM_KEYWORDS)]
    if has_work:
        bottom_candidates = sorted(bottom_candidates,
                                   key=lambda p: 100 if "qu·∫ßn t√¢y" in p["name"].lower() or "qu·∫ßn √¢u" in p["name"].lower() or "ch√¢n v√°y" in p["name"].lower() else 0,
                                   reverse=True)

    bottom = next((p for p in bottom_candidates if total + p["price"] <= max_budget), None)
    if bottom:
        outfit.append(bottom)
        total += bottom["price"]
        used_ids.add(bottom["id"])

    # 3. Th√™m √°o kho√°c n·∫øu m√πa l·∫°nh ho·∫∑c giao m√πa
    if has_winter or has_spring_autumn:
        jacket_keys = CONTEXT_KEYWORDS["winter"] if has_winter else (CONTEXT_KEYWORDS.get("autumn", []) + CONTEXT_KEYWORDS.get("spring", []))
        jacket_candidates = [p for p in sorted_prods if p["id"] not in used_ids
                             if any(fuzzy_match(k, normalize_text(p["name"])) for k in jacket_keys)]
        jacket = next((p for p in jacket_candidates if total + p["price"] <= max_budget), None)
        if jacket:
            outfit.append(jacket)

    return outfit

def build_combo(products, budget=None, max_items=5):
    if not products:
        return []
    if not budget:
        return random.sample(products[:10], min(max_items, len(products)))
    combo = []
    total = 0
    for p in sorted(products, key=lambda x: x["price"]):
        if len(combo) >= max_items:
            break
        if total + p["price"] <= budget:
            combo.append(p)
            total += p["price"]
    return combo if combo else products[:2]

# ================= YOU MAY LIKE =================

def recommend_you_may_like(products, k=4):
    high_score = [p for p in products if p.get("score", 0) >= 85]
    if not high_score:
        return []
    return random.sample(high_score, min(len(high_score), k))

# ================= SAFE JSON =================

def clean_for_json(obj):
    if isinstance(obj, float):
        if math.isnan(obj) or math.isinf(obj):
            return 0
        return obj
    if isinstance(obj, dict):
        return {k: clean_for_json(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [clean_for_json(v) for v in obj]
    return obj

# ================= MAIN FUNCTION =================

def fashion_chat(user_message: str):
    # Load DB
    vectordb = get_vectordb()

    # ∆Øu ti√™n size
    if detect_size_intent(user_message):
        height, weight = parse_height_weight(user_message)
        size = suggest_size(height, weight)
        # Query DB v·ªõi size
        query = normalize_text(user_message) + f" size {size}"
        docs = vectordb.similarity_search_with_score(query, k=50)
        products_size = [doc[0].metadata for doc in docs if doc[1] > 0.5]  # L·∫•y metadata, filter score
        products_size = filter_by_size(products_size, size)
        products_size = sort_products(products_size)
        return clean_for_json({
            "answer": f"V·ªõi chi·ªÅu cao {height}cm v√† c√¢n n·∫∑ng {weight}kg, m√¨nh khuy√™n b·∫°n ch·ªçn size {size} üëï",
            "suggested_size": size,
            "products": products_size[:6],
            "you_may_like": recommend_you_may_like(products_size)
        })

    # Detect intent
    gender = detect_gender(user_message)
    budget = detect_budget(user_message)
    wallet_level = detect_wallet_level(user_message)
    contexts = detect_contexts(user_message)

    # Build query cho DB t·ª´ user_message + intents
    query_parts = [normalize_text(user_message)]
    if gender:
        query_parts.append(gender)
    if budget:
        query_parts.append(f"gi√° d∆∞·ªõi {budget}")
    if contexts:
        query_parts.extend([", ".join(CONTEXT_KEYWORDS.get(ctx, [])) for ctx in contexts])
    query = " ".join(query_parts)

    # Query DB
    docs = vectordb.similarity_search_with_score(query, k=50)  # Top 50 ƒë·ªÉ l·ªçc ti·∫øp
    products = [doc[0].metadata for doc in docs if doc[1] > 0.5]  # L·∫•y metadata (id, name, price, etc.), filter low score

    if not products:
        # Fallback t·∫£i CSV n·∫øu DB kh√¥ng match
        products = get_products_cached()

    # Filter nh∆∞ c≈©
    products = filter_by_gender(products, gender)
    products = filter_by_budget(products, budget)
    products = filter_by_contexts(products, contexts)

    # Dedup
    seen = set()
    uniq = []
    for p in products:
        if p["id"] not in seen:
            uniq.append(p)
            seen.add(p["id"])
    products = uniq

    products = sort_products(products)

    if len(products) < 2:
        return {
            "answer": "Y√™u c·∫ßu c·ªßa b·∫°n h∆°i ƒë·∫∑c bi·ªát qu√° ho·∫∑c m√¨nh ch∆∞a t√¨m ƒë∆∞·ª£c m√≥n ph√π h·ª£p üò• H√£y chat tr·ª±c ti·∫øp v·ªõi admin ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n chi ti·∫øt h∆°n nh√©!",
            "products": [],
            "need_admin": True
        }

    # Build outfit & combo
    outfit = build_smart_outfit(products, contexts, budget)
    combo = build_combo(products, budget)

    outfit_total = sum(p["price"] for p in outfit)
    _, style_desc = get_price_category(outfit_total)

    # X√°c ƒë·ªãnh gi·ªçng ƒëi·ªáu tr·∫£ l·ªùi
    effective_wallet = wallet_level or get_price_category(outfit_total)[0]

    if effective_wallet == "gi√†u":
        answer = f"M√¨nh g·ª£i √Ω cho b·∫°n m·ªôt outfit {style_desc} c·ª±c k·ª≥ ƒë·∫≥ng c·∫•p v√† ch·∫•t l∆∞·ª£ng "
    elif effective_wallet == "kh√°":
        answer = f"ƒê√¢y l√† set ƒë·ªì {style_desc} m√† m√¨nh th·∫•y h·ª£p v·ªõi b·∫°n nh·∫•t "
    else:
        answer = f"M√¨nh ch·ªçn cho b·∫°n outfit {style_desc} "

    # Th√™m ho√†n c·∫£nh n·∫øu c√≥
    if contexts:
        context_names = {
            "winter": "m√πa ƒë√¥ng",
            "summer": "m√πa h√®",
            "autumn": "m√πa thu",
            "spring": "m√πa xu√¢n",
            "work": "ƒëi l√†m vƒÉn ph√≤ng",
            "casual": "d·∫°o ph·ªë/h√†ng ng√†y",
            "date": "h·∫πn h√≤/ƒëi ti·ªác",
            "sport": "t·∫≠p gym/th·ªÉ thao",
            "travel": "du l·ªãch",
            "home": "m·∫∑c nh√†"
        }
        displayed = [context_names.get(c, c) for c in contexts]
        answer = f"D√†nh cho {', '.join(displayed)} ‚Äì " + answer

    return clean_for_json({
        "answer": answer,
        "products": products[:6],
        "outfit_products": outfit,
        "budget_combo": combo,
        "you_may_like": recommend_you_may_like(products),
        "outfit_total_price": outfit_total,
        "need_admin": False
    })